services:
  # rabbitmq:
  #   restart: always
  #   container_name: celery_rabbitmq
  #   image: rabbitmq:latest
  #   # comment port when deploy
  #   # ports:
  #   #   - "5672:5672"
  
  # redis:
  #   restart: always
  #   container_name: celery_redis
  #   image: redis:latest
  #   # comment port when deploy
  #   # ports:
  #   #   - "6379:6379"

  # beat:
  #   depends_on:
  #     - rabbitmq
  #     - redis
  #   restart: always
  #   container_name: celery_beat
  #   image: mher/flower:latest
  #   volumes:
  #     - "./docker-images/flask/flask_project/celery_project:/data/celery_project"
  #   command: celery -A celery_project.app -b amqp://rabbitmq:5672 --result-backend redis://redis:6379/0 beat

  # worker:
  #   depends_on:
  #     - rabbitmq
  #     - redis
  #   restart: always
  #   container_name: celery_worker
  #   image: ljwdocker1989/celery_worker:latest
  #   build:
  #     context: .
  #     dockerfile: docker-images/worker/celery_worker.df
  #   volumes:
  #     - "./docker-images/flask/flask_project/celery_project:/app/celery_project"
  #     - "./docker-images/flask/flask_project/tmp:/app/tmp"
  #     - "./genome:/app/genome:ro"
  #   command: celery -A celery_project.app -b amqp://rabbitmq:5672 --result-backend redis://redis:6379/0 worker

  # flask:
  #   depends_on:
  #     - rabbitmq
  #     - redis
  #     - worker
  #   restart: always
  #   container_name: flask_app
  #   image: ljwdocker1989/flask_app:latest
  #   build:
  #     context: .
  #     dockerfile: docker-images/flask/flask.df
  #   volumes:
  #     - "./docker-images/flask/flask_project:/app"
  #   environment:
  #     CELERY_BROKER: amqp://rabbitmq:5672
  #     CELERY_BACKEND: redis://redis:6379/0
  #   # 10737418240 is 10GB
  #   command: waitress-serve --url-prefix=/workflow --max-request-body-size=10737418240 app:flaskApp

  # flower:
  #   depends_on:
  #     - rabbitmq
  #     - redis
  #     - worker
  #   restart: always
  #   container_name: celery_flower
  #   image: mher/flower:latest
  #   volumes:
  #     - "./docker-images/flask/flask_project/celery_project:/data/celery_project"
  #   command: celery -A celery_project.app -b amqp://rabbitmq:5672 --result-backend redis://redis:6379/0 flower --url_prefix=/flower

  nginx:
    depends_on:
      - flask
      - flower
      - shiny
      - chat-ui-db
    restart: always
    container_name: nginx_server
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - "./docker-images/nginx/conf:/etc/nginx/conf.d"
      - "./docker-images/nginx/html:/usr/share/nginx/html"
    # command: [nginx-debug, '-g', 'daemon off;']

  # shiny:
  #   restart: always
  #   container_name: shiny_server
  #   image: ljwdocker1989/shiny_server:latest
  #   build:
  #     context: .
  #     dockerfile: docker-images/shiny/shiny_server.df
  #   volumes:
  #     - "./docker-images/shiny/apps:/srv/shiny-server"
  #     - "./docker-images/shiny/conf:/etc/shiny-server"
  #     - "./docker-images/shiny/logs:/var/log/shiny-server"

  chat-ui-db:
    depends_on:
      - TEI
      - TGI
    restart: always
    container_name: chat-ui-db
    image: ghcr.io/huggingface/chat-ui-db:latest
    volumes:
      - "./docker-images/chat-ui/.env.local:/app/.env.local"
      - "./docker-images/chat-ui/chat:/data"
    ports:
      - "3000:3000"

  TGI:
    restart: always
    container_name: chat-ui-TGI
    image: ghcr.io/huggingface/text-generation-inference:latest
    volumes:
      - "./docker-images/chat-ui/TGI:/data"
    # allow enough shared memory
    shm_size: '32gb'
    command: --model-id https//hf-mirror.com/microsoft/Phi-3-mini-4k-instruct-gguf

  TEI:
    restart: always
    container_name: chat-ui-TEI
    image: ghcr.io/huggingface/text-embeddings-inference:latest
    volumes:
      - "./docker-images/chat-ui/TEI:/data"
    command: --model-id https//hf-mirror.com/BAAI/bge-large-en-v1.5
