services:
  rabbitmq:
    restart: always
    container_name: celery_rabbitmq
    image: rabbitmq:latest
    # comment port when deploy
    # ports:
    #   - "5672:5672"
  
  redis:
    restart: always
    container_name: celery_redis
    image: redis:latest
    # comment port when deploy
    # ports:
    #   - "6379:6379"

  beat:
    depends_on:
      - rabbitmq
      - redis
    restart: always
    container_name: celery_beat
    image: ljwdocker1989/celery_flower:latest
    build:
      context: .
      dockerfile: celery_flower.df
    command: celery -A celery_project.app -b amqp://rabbitmq:5672 --result-backend redis://redis:6379/0 beat

  worker:
    depends_on:
      - rabbitmq
      - redis
    restart: always
    container_name: celery_worker
    image: ljwdocker1989/celery_worker:latest
    build:
      context: .
      dockerfile: celery_worker.df
    volumes:
      - "./flask_project/tmp:/app/tmp"
      - "./genome:/app/genome:ro"
    command: celery -A celery_project.app -b amqp://rabbitmq:5672 --result-backend redis://redis:6379/0 worker

  flask:
    depends_on:
      - rabbitmq
      - redis
      - worker
    restart: always
    container_name: flask_app
    image: ljwdocker1989/flask_app:latest
    build:
      context: .
      dockerfile: flask.df
    volumes:
      - "./flask_project/tmp:/app/tmp"
    environment:
      CELERY_BROKER: amqp://rabbitmq:5672
      CELERY_BACKEND: redis://redis:6379/0
    command: waitress-serve --url-prefix=/workflow app:flaskApp

  flower:
    depends_on:
      - rabbitmq
      - redis
      - worker
    restart: always
    container_name: celery_flower
    image: ljwdocker1989/celery_flower:latest
    build:
      context: .
      dockerfile: celery_flower.df
    command: celery -A celery_project.app -b amqp://rabbitmq:5672 --result-backend redis://redis:6379/0 flower --url_prefix=/flower

  nginx:
    depends_on:
      - flask
      - flower
      - shiny
    restart: always
    container_name: flask_nginx
    image: nginx:latest
    ports:
      - "80:80"
      # - "443:443"
    volumes:
      - "./nginx/conf/:/etc/nginx/conf.d/:ro"
      - "./nginx/html/:/usr/share/nginx/html/:ro"
      # - "./openssl/:/etc/ssl/:ro"
      # - "./certbot/letsencrypt/:/etc/letsencrypt/:ro"
    # command: [nginx-debug, '-g', 'daemon off;']

  shiny:
    restart: always
    container_name: shiny_server
    image: ljwdocker1989/shiny_server:latest
    build:
      context: .
      dockerfile: ./shiny_server.df
    volumes:
      - "./shinyApps/:/srv/shiny-server/:rw"

  chat-ui-db:
    depends_on:
      - Phi-3-mini-4k-instruct-gguf
    restart: always
    container_name: chat-ui-db
    image: ghcr.io/huggingface/chat-ui-db
    volumes:
      - "./chat-ui/.env.local:/app/.env.local"
      - "./chat-ui/data:/data"
    environment:
      PUBLIC_ORIGIN: "http://qiangwulab.sjtu.edu.cn:80"

  Phi-3-mini-4k-instruct-gguf:
    restart: always
    container_name: Phi-3-mini-4k-instruct-gguf
    image: ghcr.io/ggerganov/llama.cpp:server
    volumes:
      - "./chat-ui/llama.cpp:/models"
    command: -m /models/Phi-3-mini-4k-instruct-q4.gguf --host 0.0.0.0 -c 4096
